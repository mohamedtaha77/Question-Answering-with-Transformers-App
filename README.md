# â“ Question Answering Web App

This is a clean and interactive **Question Answering (QA) App** built with **Streamlit**.  
It extracts answers from a given paragraph using **Transformer-based models**.

Three models are implemented and fully deployed in the UI:
- **DistilBERT**
- **BERT**
- **RoBERTa**

---

## ğŸ“Œ Features

âœ… Extracts answers from context paragraphs using Transformers  
âœ… Supports **DistilBERT**, **BERT-base**, and **BERT-large** models  
âœ… Uses HuggingFaceâ€™s `pipeline("question-answering")`  
âœ… Streamlined interface for paragraph and question input  
âœ… Displays extracted **answer**, **score**, and **answer span**  
âœ… Fully interactive **Streamlit UI** with dropdown model selection  
âœ… Optimized loading using `torch.no_grad()` and caching  
âœ… Complete training and inference workflow documented

---

## ğŸ”— Live Demo

Try the app live here:  
ğŸ‘‰ [https://question-answering-with-transformers-77.streamlit.app/](https://question-answering-with-transformers-77.streamlit.app/)

---

## ğŸ“ Files Included

| File/Folder | Description |
|-------------|-------------|
| `app/` | Contains `app.py`, the Streamlit web app for transformer-based QA |
| `notebooks/` | Jupyter notebooks for training and saving each transformer QA model |
| `qa_model_bert-base-uncased/` | Saved model and tokenizer files for fine-tuned BERT Base |
| `qa_model_distilbert-base-uncased/` | Saved model and tokenizer files for fine-tuned DistilBERT |
| `qa_model_roberta-base/` | Saved model and tokenizer files for fine-tuned RoBERTa |
| `results/` | Evaluation results and visualizations |
| `scripts/` | Python scripts for preprocessing, training, or utilities |
| `.gitattributes` | Git configuration file for handling line endings |
| `README.md` | Project documentation and overview |
| `requirements.txt` | List of required Python packages for setup |


---

## ğŸš€ How to Run Locally

1. **Clone the repo**:

```bash
git clone https://github.com/yourusername/question-answering-transformers.git
cd question-answering-transformers
```

2. **(Optional) Create a virtual environment**:

```bash
python -m venv venv
venv\Scripts\activate  # on Windows
```

3. **Install dependencies**:

```bash
pip install -r requirements.txt
```

4. **Run the app**:

```bash
streamlit run app.py
```

---

## ğŸŒ Deployment (Optional)

To deploy on **Streamlit Cloud**:

- Go to [https://streamlit.io/cloud](https://streamlit.io/cloud)
- Connect your GitHub repo
- Set the main file to: `app.py`
- Make sure your models are in the `model/` folder or downloaded via HuggingFace inside the script

---

## ğŸ““ Notebook Workflow

Due to resource constraints, the training for each model was conducted in **separate notebooks**, ensuring stability and avoiding runtime crashes. Each notebook follows the same workflow:

- Load pre-trained model from HuggingFace (`distilbert-base-uncased`, `bert-base-uncased`, `bert-large-uncased`)
- Preprocess SQuAD-like QA data (context, question, answer)
- Fine-tune using HuggingFace Trainer API
- Save model weights and tokenizer locally for deployment
- Load for inference using the QA pipeline

---
